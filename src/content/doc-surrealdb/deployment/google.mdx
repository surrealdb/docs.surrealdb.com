---
sidebar_position: 7
sidebar_label: Deploy on Google GKE
title: Deploy on Google GKE | Deployment
description: How to set up a SurrealDB cluster backed by TiKV on Google Kubernetes Engine.
---

import Image from "@components/Image.astro";
import LightLogo from "@img/icon/light/google.png";
import DarkLogo from "@img/icon/dark/google.png";

<Image
    alt="Google Kubernetes Engine"
    width={100}
    className="flag-image-title"
    src={{
        light: LightLogo,
        dark: DarkLogo,
    }}
/>

# Deploy on Google Kubernetes Engine (GKE)

This article will guide you through the process of setting up a highly available SurrealDB cluster backed by TiKV on a GKE Autopilot cluster.

## What is GKE?

[Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine) is a managed Kubernetes service offered by Google Cloud Platform. In this guide we will create a [GKE Autopilot](https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview) cluster, which removes the need to manage the underlying compute nodes.

## What is TiKV?
[TiKV](https://tikv.org/) is a cloud-native transactional key/value store built by PingCAP and that integrates well with Kubernetes thanks to their [TiDB operator](https://github.com/pingcap/tidb-operator).

## Prerequisites

In order for you to complete this tutorial you'll need:

- An account on [Google Cloud Platform](https://cloud.google.com/)
- The [gcloud CLI](https://cloud.google.com/sdk/gcloud) installed and configured
- `kubectl` with gcloud integration for accessing the GKE cluster. Installation [here](https://cloud.google.com/kubernetes-engine/docs/tutorials/cluster-access-for-kubectl)
- `helm` : To install SurrealDB server and TiKV
- [`Surreal CLI`](/install) : To interact with the SurrealDB server

## Create GKE Cluster

1. Choose the target project and region. List them with these commands:

```bash title="List projects and regions"
gcloud projects list

gcloud compute regions list --project PROJECT_ID
```

2. Run the following command to create a cluster replacing the REGION and PROJECT_ID for your desired values:

```bash title="Create new GKE autopilot Cluster"
gcloud container clusters create-auto surrealdb-guide --region REGION --project PROJECT_ID
```

3. After the creation finishes, configure kubectl to connect to the new cluster:

```bash title="kubectl configuration"
gcloud container clusters get-credentials surrealdb-guide --region REGION --project PROJECT_ID
```

## Deploy TiDB operator

Now that we have a Kubernetes cluster, we can deploy the TiDB operator. TiDB operator is a Kubernetes operator that manages the lifecycle of TiDB clusters deployed to Kubernetes.

You can deploy it following these steps:

1. Install CRDS:

```bash title="CRDS installation"
kubectl create -f https://raw.githubusercontent.com/pingcap/tidb-operator/v1.5.0/manifests/crd.yaml
```

2. Install TiDB operator Helm chart:


```bash title="Helm chart installation"
$ helm repo add pingcap https://charts.pingcap.org
$ helm repo update
$ helm install \
	-n tidb-operator \
    --create-namespace \
	tidb-operator \
	pingcap/tidb-operator \
	--version v1.5.0
```
3. Verify that the Pods are running:

```bash title="Get pod status"
kubectl get pods --namespace tidb-operator -l app.kubernetes.io/instance=tidb-operator
NAME                                       READY   STATUS    RESTARTS   AGE
tidb-controller-manager-56f49794d7-hnfz7   1/1     Running   0          20s
tidb-scheduler-8655bcbc86-66h2d            2/2     Running   0          20s
```

## Create TiDB cluster

Now that we have the TiDB Operator running, it’s time to define a TiDB Cluster and let the Operator do the rest.

1. Create a local file named `tikv-cluster.yaml` with this content:

```yaml title="tikv-cluster.yaml"
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: sdb-datastore
spec:
  version: v6.5.0
  timezone: UTC
  configUpdateStrategy: RollingUpdate
  pvReclaimPolicy: Delete
  enableDynamicConfiguration: true
  schedulerName: default-scheduler
  topologySpreadConstraints:
  - topologyKey: topology.kubernetes.io/zone
  helper:
    image: alpine:3.16.0
  pd:
    baseImage: pingcap/pd
    maxFailoverCount: 0
    replicas: 3
    storageClassName: premium-rwo
    requests:
      cpu: 500m
      storage: 10Gi
      memory: 1Gi
    config: |
      [dashboard]
        internal-proxy = true
      [replication]
        location-labels = ["topology.kubernetes.io/zone", "kubernetes.io/hostname"]
        max-replicas = 3
    nodeSelector:
      dedicated: pd
    tolerations:
    - effect: NoSchedule
      key: dedicated
      operator: Equal
      value: pd
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values:
              - pd
          topologyKey: kubernetes.io/hostname
  tikv:
    baseImage: pingcap/tikv
    maxFailoverCount: 0
    replicas: 3
    storageClassName: premium-rwo
    requests:
      cpu: 1
      storage: 10Gi
      memory: 2Gi
    config: {}
    nodeSelector:
      dedicated: tikv
    tolerations:
    - effect: NoSchedule
      key: dedicated
      operator: Equal
      value: tikv
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app.kubernetes.io/component
              operator: In
              values:
              - tikv
          topologyKey: kubernetes.io/hostname
  tidb:
    replicas: 0
```

2. Create the TiDB cluster:

```bash title="Apply cluster def"
kubectl apply -f tikv-cluster.yaml
```

3. Check the cluster status and wait until it’s ready:

```bash title="Get status for cluster"
kubectl get tidbcluster
NAME             READY   PD                  STORAGE   READY   DESIRE   TIKV                  STORAGE   READY   DESIRE   TIDB                  READY   DESIRE   AGE
sdb-datastore   True    pingcap/pd:v6.5.0   10Gi      3       3        pingcap/tikv:v6.5.0   10Gi      3       3        pingcap/tidb:v6.5.0           0        5m
```

## Deploy SurrealDB

Now that we have a TiDB cluster running, we can deploy SurrealDB using the official Helm chart.

The deployment will use the latest SurrealDB Docker image and make it accessible on the internet.

1. Get the TiKV PD service URL:

```bash title="get TiKV URL"
kubectl get svc/sdb-datastore-pd
NAME       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
sdb-datastore-pd   ClusterIP   10.96.208.25   <none>        2379/TCP   10h

export TIKV_URL=tikv://sdb-datastore-pd:2379
```
2. Install the SurrealDB Helm chart with the TIKV_URL defined above and with auth disabled so we can create the initial credentials:

```bash title="Install Helm chart"
helm repo add surrealdb https://helm.surrealdb.com
$ helm repo update
$ helm install \
    --set surrealdb.path=$TIKV_URL \
    --set surrealdb.auth=false \
    --set ingress.enabled=true \
    --set image.tag=latest \
    surrealdb-tikv surrealdb/surrealdb
```

3. Wait until the Ingress resource has an `ADDRESS` assigned:

```bash title="Wait for Ingress ADDRESS"
kubectl get ingress surrealdb-tikv
NAME             CLASS    HOSTS   ADDRESS         PORTS   AGE
surrealdb-tikv   <none>   *       34.160.82.177   80      5m
```

4. Connect to the cluster and define the initial credentials:

```bash title="Connect then define user"
$ export SURREALDB_URL=http://$(kubectl get ingress surrealdb-tikv -o json | jq -r .status.loadBalancer.ingress[0].ip)
$ surreal sql -e $SURREALDB_URL
> DEFINE USER root ON ROOT PASSWORD 'StrongSecretPassword!' ROLES OWNER;
```

5. Verify you can connect to the database with the new credentials:

```bash title="Connect as defined user"
$ surreal sql -u root -p 'StrongSecretPassword!' -e $SURREALDB_URL
> INFO FOR ROOT
[{ accesses: {  }, namespaces: {  }, nodes: { "0e87c953-68d7-40e1-9090-3dfc404af25e": 'NODE 0e87c953-68d7-40e1-9090-3dfc404af25e SEEN 1742869518357 ACTIVE' }, system: { available_parallelism: 14, cpu_usage: 4.321133613586426f, load_average: [2.2265625f, 2.2138671875f, 2.044921875f], memory_allocated: 13428527, memory_usage: 154812416, physical_cores: 14, threads: 32 }, users: { root: "DEFINE USER root ON ROOT PASSHASH '...' ROLES OWNER DURATION FOR TOKEN 1h, FOR SESSION NONE" } }]
```

5. Now that the initial credentials have been created, enable authentication:

```bash title="Update SurrealDB Helm chart"
helm upgrade \
    --set surrealdb.path=$TIKV_URL \
    --set surrealdb.auth=true \
    --set ingress.enabled=true \
    --set image.tag=latest \
    surrealdb-tikv surrealdb/surrealdb
```

## Cleanup

Run the following commands to delete the Kubernetes resources and the GKE cluster:

```bash title="Cleanup command"
kubectl delete tidbcluster sdb-datastore
helm uninstall surrealdb-tikv
helm -n tidb-operator uninstall tidb-operator
gcloud container clusters delete surrealdb-guide --region REGION --project PROJECT_ID
```


# Deploy SurrealDB + TiKV on **any** Kubernetes (with GKE Autopilot as the default)

This article will guide you through the process of setting up a highly available SurrealDB cluster backed by TiKV on a GKE Autopilot cluster. It is a parameter‑driven guide, so you only edit a handful of variables to deploy the cluster.



<table>
  <thead>
    <tr>
      <th>Variable</th>
      <th>Default</th>
      <th>Purpose</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>PROJECT_ID</code></td>
      <td>—</td>
      <td>Your GCP project (required for GKE)</td>
    </tr>
    <tr>
      <td><code>REGION</code></td>
      <td><code>europe‑west2</code></td>
      <td>GKE cluster region</td>
    </tr>
    <tr>
      <td><code>CLUSTER_NAME</code></td>
      <td><code>surrealdb-guide</code></td>
      <td>Cluster name</td>
    </tr>
    <tr>
      <td><code>TIDB_VERSION</code></td>
      <td><code>v1.5.0</code></td>
      <td>TiDB Operator & CRD version</td>
    </tr>
    <tr>
      <td><code>SURDB_TAG</code></td>
      <td><code>1.5.2</code></td>
      <td>SurrealDB Docker tag</td>
    </tr>
    <tr>
      <td><code>STORAGE_CLASS</code></td>
      <td><code>premium-rwo</code></td>
      <td>StorageClass used by TiKV</td>
    </tr>
  </tbody>
</table>

```bash
# Set variables ONCE
export PROJECT_ID=<your‑gcp‑project>
export REGION=europe-west2
export CLUSTER_NAME=surrealdb-guide
export TIDB_VERSION=v1.5.0
export SURDB_TAG=1.5.2
export STORAGE_CLASS=premium-rwo
````

##  Create / attach to the Kubernetes cluster

GKE Autopilot (recommended)

```bash
# Authenticate & project
gcloud config set project "$PROJECT_ID"

# Create the Autopilot cluster (no node sizing needed)
gcloud container clusters create-auto "$CLUSTER_NAME" \
  --region "$REGION"

# Kube‑config
gcloud container clusters get-credentials "$CLUSTER_NAME" \
  --region "$REGION"
```


Bring‑your‑own cluster

> Skip this step if you already have a Kubernetes ≥ 1.27 cluster with:
>
> * LoadBalancer service type
> * A default `StorageClass` (or set `STORAGE_CLASS` above)


## Install TiDB Operator (manages TiKV)

```bash
# CRDs
kubectl apply -f \
  "https://raw.githubusercontent.com/pingcap/tidb-operator/${TIDB_VERSION}/manifests/crd.yaml"

# Helm chart
helm repo add pingcap https://charts.pingcap.org
helm repo update
helm upgrade --install tidb-operator pingcap/tidb-operator \
  --namespace tidb-operator --create-namespace \
  --version "$TIDB_VERSION"
```


## Deploy the TiKV cluster

Save **`tikv-cluster.yaml.tpl`**:

```yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: sdb-datastore
spec:
  version: v6.5.0
  timezone: UTC
  pvReclaimPolicy: Delete
  enableDynamicConfiguration: true
  schedulerName: default-scheduler

  pd:
    baseImage: pingcap/pd
    replicas: 3
    storageClassName: ${STORAGE_CLASS}
    requests:
      cpu: 500m
      memory: 1Gi
      storage: 10Gi

  tikv:
    baseImage: pingcap/tikv
    replicas: 3
    storageClassName: ${STORAGE_CLASS}
    requests:
      cpu: 1
      memory: 2Gi
      storage: 10Gi

  tidb:
    replicas: 0
```

Then apply it with your env vars:

```bash
envsubst < tikv-cluster.yaml.tpl | kubectl apply -f -
kubectl wait --for=condition=Ready tidbcluster/sdb-datastore --timeout=15m
```


## Install SurrealDB (Helm values file)

Create `surreal-values.yaml`:

```yaml
surrealdb:
  path: "tikv://sdb-datastore-pd:2379"
  auth: false                 # toggled in step 5
image:
  tag: "${SURDB_TAG}"
ingress:
  enabled: true
```

Install:

```bash
helm repo add surrealdb https://helm.surrealdb.com
helm repo update
helm upgrade --install surrealdb-tikv surrealdb/surrealdb \
  -f surreal-values.yaml
```

Wait for the ingress to receive an external IP:

```bash
kubectl get ingress surrealdb-tikv -w
export SURDB_URL="http://$(kubectl get ingress surrealdb-tikv \
  -o jsonpath='{.status.loadBalancer.ingress[0].ip}')"
```


## Bootstrap SurrealDB credentials

```bash
# Connect unauthenticated (one‑time)
surreal sql -e "$SURDB_URL"

# Create a root user
> DEFINE USER root ON ROOT PASSWORD 'StrongSecretPassword!' ROLES OWNER;
```

Now re‑enable auth:

```bash
helm upgrade surrealdb-tikv surrealdb/surrealdb \
  -f surreal-values.yaml \
  --set surrealdb.auth=true
```

Verify:

```bash
surreal sql -u root -p 'StrongSecretPassword!' -e "$SURDB_URL"
```


## Cleanup

```bash
kubectl delete tidbcluster sdb-datastore
helm uninstall surrealdb-tikv
helm -n tidb-operator uninstall tidb-operator

# If you created a GKE cluster:
gcloud container clusters delete "$CLUSTER_NAME" \
  --region "$REGION" --quiet
```

---

### Automation scripts (optional)

Add these to `/scripts` if you prefer one‑liners:

| Script         | What it does |
| -------------- | ------------ |
| `provision.sh` | steps 0–4    |
| `bootstrap.sh` | step 5       |
| `cleanup.sh`   | step 6       |

Each script simply reproduces the sections above with `bash -euo pipefail` for safety.

---

## Done 🎉

Because **all versions & paths live in variables or YAML files**, maintaining the guide now means bumping a couple of numbers instead of editing dozens of snippets.

```
```
