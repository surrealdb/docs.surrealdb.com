---
sidebar_position: 5
sidebar_label: Fastembed
title: Fastembed | Embeddings
description: This section contains information about using Fastembed to retrieve embeddings to store in SurrealDB
---

import Label from "@components/shared/Label.astro";
import Tabs from "@components/Tabs/Tabs.astro";
import TabItem from "@components/Tabs/TabItem.astro";

# Fastembed

Fastembed is a library that allows you to generate vector embeddings locally, without needing an API key or calling into an external service.

Fastembed libraries are available for the following languages:

* [Python](https://github.com/qdrant/fastembed)
* [Rust](https://crates.io/crates/fastembed)
* [Go](https://github.com/Anush008/fastembed-go)
* [JavaScript](https://github.com/Anush008/fastembed-js)

## Overview of available models

The following is an overview of the models available for Fastembed. The defaults generally seen per use case are:

* Text embedding: BGESmallENV15
* Image embedding: ClipVitB32
* Sparse text embedding: 

### MiniLM Series

Fast general-purpose embeddings. Choose L6 for speed, L12 for quality. Ideal for semantic search, clustering, and similarity tasks.

A "quantized" model means that it is optimised for faster inference and lower memory usage, often with minimal quality loss.

| Model name      | Embedding size | Description                                         |
| --------------- | -------------- | --------------------------------------------------- |
| AllMiniLML6V2   | 384            | Sentence Transformer model, MiniLM-L6-v2            |
| AllMiniLML6V2Q  | 384            | Quantized Sentence Transformer model, MiniLM-L6-v2  |
| AllMiniLML12V2  | 384            | Sentence Transformer model, MiniLM-L12-v2           |
| AllMiniLML12V2Q | 384            | Quantized Sentence Transformer model, MiniLM-L12-v2 |

### BGE Series 

Used for dense retrieval and semantic similarity. BGESmallENV15 is optimized for speed and tends to be the default choice for many applications.

| Model name     | Embedding size | Description                                                  |
| -------------- | -------------- | ------------------------------------------------------------ |
| BGEBaseENV15   | 768            | v1.5 release of the base English model                       |
| BGEBaseENV15Q  | 768            | Quantized v1.5 release of the base English model             |
| BGELargeENV15  | 1024           | v1.5 release of the large English model                      |
| BGELargeENV15Q | 1024           | Quantized v1.5 release of the large English model            |
| BGESmallENV15  | 384            | v1.5 release of the fast and default English model           |
| BGESmallENV15Q | 384            | Quantized v1.5 release of the fast and default English model |


### Nomic Embed Text

Used for large context window embeddings.

Optimized for long-context English text (8K tokens). v1.5 improves quality over v1.

| Model name         | Embedding size | Description                                                     |
| ------------------ | -------------- | --------------------------------------------------------------- |
| NomicEmbedTextV1   | 768            | 8192 context length english model                               |
| NomicEmbedTextV15  | 768            | v1.5 release of the 8192 context length english model           |
| NomicEmbedTextV15Q | 768            | Quantized v1.5 release of the 8192 context length english model |


### Paraphrase Models

Used for paraphrase detection and multilingual similarity. Ideal for sentence equivalence and semantic matching tasks.

| Model name               | Embedding size | Description                                                              |
| ------------------------ | -------------- | ------------------------------------------------------------------------ |
| ParaphraseMLMiniLML12V2  | 384            | Multi-lingual model                                                      |
| ParaphraseMLMiniLML12V2Q | 384            | Quantized Multi-lingual model                                            |
| ParaphraseMLMpnetBaseV2  | 768            | Sentence-transformers model for tasks like clustering or semantic search, based on the MPNet architecture. |


### Chinese BGE Models

| Model name    | Embedding size | Description                             |
| ------------- | -------------- | --------------------------------------- |
| BGESmallZHV15 | 512            | v1.5 release of the small Chinese model |
| BGELargeZHV15 | 1024           | v1.5 release of the large Chinese model |


### ModernBert and Multilingual E5

Used for context-rich multilingual embeddings. Great for cross-language retrieval and nuanced contextual understanding.

| Model name           | Embedding size | Description                                    |
| -------------------- | -------------- | ---------------------------------------------- |
| ModernBertEmbedLarge | 1024           | Large model of ModernBert Text Embeddings      |
| MultilingualE5Small  | 384            | Small model of multilingual E5 Text Embeddings |
| MultilingualE5Base   | 768            | Base model of multilingual E5 Text Embeddings  |
| MultilingualE5Large  | 1024           | Large model of multilingual E5 Text Embeddings |


### Mxbai and GTE

Used for high-quality English/multilingual embeddings.

| Model name         | Embedding size | Description                                                |
| ------------------ | -------------- | ---------------------------------------------------------- |
| MxbaiEmbedLargeV1  | 1024           | Large English embedding model from MixedBreed.ai           |
| MxbaiEmbedLargeV1Q | 1024           | Quantized Large English embedding model from MixedBreed.ai |
| GTEBaseENV15       | 768            | Base multilingual embedding model from Alibaba            |
| GTEBaseENV15Q      | 768            | Quantized base multilingual embedding model from Alibaba  |
| GTELargeENV15      | 1024           | Large multilingual embedding model from Alibaba            |
| GTELargeENV15Q     | 1024           | Quantized large multilingual embedding model from Alibaba  |


### CLIP and Code Models

Use CLIP for image-text matching, Jina for code search and retrieval. JinaEmbeddingsV2BaseCode is optimised for embedding code snippets.

| Model name               | Embedding size | Description                         |
| ------------------------ | -------------- | ----------------------------------- |
| ClipVitB32               | 512            | CLIP text encoder based on ViT-B/32 |
| JinaEmbeddingsV2BaseCode | 768            | Jina embeddings v2 base code        |

## Language-specific examples

The following examples show how to use fastembed together with the SurrealDB SDK for the language of your choice.

<Tabs>
<TabItem label="Rust">

First add a few crates to Cargo.toml with the following command:

```bash
cargo add anyhow fastembed serde tokio surrealdb --features surrealdb/kv-mem
```



```rust
use anyhow::Error;
use fastembed::{EmbeddingModel, InitOptions, TextEmbedding};
use serde::{Deserialize, Serialize};
use surrealdb::{
    RecordId, Surreal, Value,
    engine::any::{Any, connect},
};

#[derive(Serialize)]
struct DocumentInput {
    text: String,
    embedding: Vec<f32>,
}

#[derive(Debug, Deserialize)]
struct Document {
    id: RecordId,
    embedding: Vec<f32>,
    text: String,
}

fn create_embed(input: &str, model: &mut TextEmbedding) -> Result<Vec<f32>, Error> {
    let mut result = model.embed(vec![input], None)?;
    Ok(result.remove(0))
}

async fn store_doc(input: &str, db: &Surreal<Any>, model: &mut TextEmbedding) -> Result<(), Error> {
    let embeds = create_embed(input, model)?;

    let _in_db = db
        .create::<Option<Document>>("document")
        .content(DocumentInput {
            text: input.into(),
            embedding: embeds,
        })
        .await?;
    Ok(())
}

async fn test_embed(
    input: &str,
    db: &Surreal<Any>,
    model: &mut TextEmbedding,
) -> Result<(), Error> {
    let embeds = create_embed(input, model)?;

    let val = db
        .query(
            "SELECT 
        text, 
        vector::distance::knn() AS distance 
        FROM document
        WHERE embedding <|3,COSINE|> $embeds
        ORDER BY distance;",
        )
        .bind(("embeds", embeds))
        .await?
        .take::<Value>(0)?;
    println!("{val}\n");
    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Error> {
    // Default model
    let mut model = TextEmbedding::try_new(InitOptions::new(EmbeddingModel::BGESmallENV15))?;

    let db = connect("memory").await?;

    db.use_ns("ns").use_db("db").await?;

    db
        .query(
            "DEFINE TABLE document;
             DEFINE FIELD text ON document TYPE string;
             DEFINE FIELD embedding ON document TYPE array<float>;
             // Uncomment this to use HNSW index, ensure that number after DIMENSION matches size of embedding
             // DEFINE INDEX hnsw_embed ON document FIELDS embedding HNSW DIMENSION 384 DIST COSINE;
             ",
        )
        .await?;

    for input in [
        // Cities
        "Calgary is a city in the Canadian province of Alberta.",
        "Ljubljana is the capital and largest city of Slovenia.",
        // Historical / mythological figures
        "Xenophon of Athens was a Greek military leader, philosopher, and historian.",
        "King Arthur was a mythical king in the mythology of Great Britain.",
        // Planets
        "Venus is the second planet from the Sun.",
        "Ceres is a dwarf planet in the middle main asteroid belt between the orbits of Mars and Jupiter.",
        // Languages
        "Manx is a Gaelic language of the insular Celtic branch of the Celtic language family",
        "Interlingue, originally Occidental, is an international auxiliary language created in 1922.",
        // Sea animals
        "Octopuses have a complex nervous system and are among the most intelligent and behaviourally diverse invertebrates.",
        "Clams have no central nervous system at all and are near to plants in intelligence.",
    ] {
        store_doc(input, &db, &mut model).await?
    }

    println!("Edmonton is closest to:");
    test_embed("Edmonton", &db, &mut model).await?;

    println!("Merlin is closest to:");
    test_embed("Merlin", &db, &mut model).await?;

    println!("Earth is closest to:");
    test_embed("Earth", &db, &mut model).await?;

    println!("Irish is closest to:");
    test_embed("Irish language", &db, &mut model).await?;

    println!("Squid are closest to:");
    test_embed("Squid", &db, &mut model).await?;

    Ok(())
}
```

</TabItem>
</Tabs>