---
sidebar_position: 0
sidebar_label: Embedding models
title: Embeddings models
description: This section contains information about different embedding models you can use with SurrealDB.
---

import Tabs from "@components/Tabs/Tabs.astro";
import TabItem from "@components/Tabs/TabItem.astro";

# Embeddings

SurrealDB offers comprehensive support for vector embeddings, enabling powerful semantic search and machine learning capabilities across your data. Through integrations with leading embedding providers, you can easily store, index and query high-dimensional vectors alongside your regular data.

<Tabs>
<TabItem label="LangChain">

More details and providers in [LangChain Embedding models](https://python.langchain.com/docs/integrations/text_embedding/) documentation.

<Tabs>
<TabItem label="Ollama">

## Ollama

```python
from langchain_ollama import OllamaEmbeddings

vector_store = SurrealDBVectorStore(
    OllamaEmbeddings(model="all-minilm:22m"),
    conn
)
```

More [Ollama embedding models](https://ollama.com/search?c=embedding) in their documentation.

</TabItem>
<TabItem label="OpenAI">

## OpenAI

Requires `OPENAI_API_KEY` environment variable.

```python
from langchain_openai import OpenAIEmbeddings

vector_store = SurrealDBVectorStore(
    OpenAIEmbeddings(model="text-embedding-3-large"),
    conn
)
```
</TabItem>
<TabItem label="Mistral">

## Mistral

Requires `MISTRALAI_API_KEY` environment variable.

```python
from langchain_mistralai import MistralAIEmbeddings

vector_store = SurrealDBVectorStore(
    MistralAIEmbeddings(model="mistral-embed"),
    conn
)
```
</TabItem>
<TabItem label="HuggingFace / SentenceTransformer">

## HuggingFace / SentenceTransformer

```python
from langchain_huggingface import HuggingFaceEmbeddings

vector_store = SurrealDBVectorStore(
    HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    ),
    conn
)
```

More [SentenceTransformer models](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) in their documentation.

</TabItem>

<TabItem label="AWS Bedrock">

## AWS Bedrock

```python
from langchain_aws import BedrockEmbeddings

vector_store = SurrealDBVectorStore(
    BedrockEmbeddings(model_id="amazon.titan-embed-text-v2:0"),
    conn
)
```

</TabItem>
<TabItem label="Gemini">

## Gemini

Requires `GOOGLE_API_KEY` environment variable.

```python
from langchain_google_genai import GoogleGenerativeAIEmbeddings

vector_store = SurrealDBVectorStore(
    GoogleGenerativeAIEmbeddings(model="models/embedding-001"),
    conn
)
```

</TabItem>
</Tabs> {/* LangChain tabs */}

<br />

Find an example in [Minimal LangChain chatbot example with vector and graph](/blog/minimal-langchain-chatbot-example-with-vector-and-graph).

</TabItem>

<TabItem label="Python">

<Tabs>
<TabItem label="Ollama">

## Ollama

```python
import ollama

embedding = ollama.embed(model="all-minilm:22m", input=text)
conn.create("documents", { "content": text, "embedding": embedding })
```

More [Ollama embedding models](https://ollama.com/search?c=embedding) in their documentation.

</TabItem>
<TabItem label="OpenAI">

## OpenAI

```python
from openai import OpenAI
client = OpenAI()

text = "Your text string"
response = client.embeddings.create(
    input=text,
    model="text-embedding-3-small"
)

conn.create("documents", { "content": text, "embedding": response.data[0].embedding })
```

More info in [OpenAI embeddings](https://platform.openai.com/docs/guides/embeddings?lang=python) documentation.

</TabItem>
<TabItem label="Sentence Transformers">

## Sentence Transformers

```python
from sentence_transformers import SentenceTransformer

st = SentenceTransformer("all-MiniLM-L6-v2")
embedding = st.encode(text).tolist()
conn.create("documents", { "content": text, "embedding": embedding })
```

More [SentenceTransformer models](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html) in their documentation.

</TabItem>
</Tabs> {/* Python tabs */}

<br />

Examples above assume you have a DB connection like this:

```python
conn = Surreal("localhost")
conn.signin({"username": "root", "password": "secret"})
conn.use("test_ns", "test_db")
```

</TabItem>
<TabItem label="Rust">

<Tabs>
<TabItem label="Mistral">

## Mistral

```rust
use mistralai_client::v1::{client::Client, constants::EmbedModel};

static KEY = std::env::var("MISTRAL_API_KEY").unwrap();

// ...

let client = Client::new(Some(KEY.to_string()), None, None, None)?;
let input = vec!["Joram is the main character in the Darksword Trilogy.".to_string()];

let result = client.embeddings_async(MODEL, input, None).await?;
println!("{:?}", result);
```

Find a full example in [Semantic search in Rust with SurrealDB and Mistral AI](/blog/semantic-search-in-rust-with-surrealdb-and-mistral-ai#generate-mistral-ai-embeddings).

</TabItem>
<TabItem label="Ollama">

## Ollama

```rust
use ollama_rs::{Ollama, generation::embeddings::GenerateEmbeddingsRequest};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let ollama = Ollama::default();

    let model = "all-minilm:22m".to_string()
    let prompt = "this is your input text".to_string();

    let request = GenerateEmbeddingsRequest::new(model, prompt);
    let response = ollama.generate_embeddings(request).await?;

    println!("Generated embeddings (first 5): {:?}", &response.embeddings[..5]);
    println!("Embedding vector length: {}", response.embeddings.len());

    Ok(())
}
```

</TabItem>
<TabItem label="SentenceTransformer">

## SentenceTransformer

```rust
use rust_bert::sentence_embeddings::{
    SentenceEmbeddingsBuilder, SentenceEmbeddingsModelType,
};

fn main() -> anyhow::Result<()> {
    // Set up the model builder, specifying the model type
    let model = SentenceEmbeddingsBuilder::remote(
        SentenceEmbeddingsModelType::AllMiniLmL6V2
    ).create_model()?;

    // Define the sentences to embed
    let sentences = [
        "this is your text",
        "you can encode more than one in batch"
    ];

    // Generate the embeddings
    let embeddings = model.encode(&sentences)?;

    // Print the results
    for (i, embedding) in embeddings.iter().enumerate() {
        // Truncate for display purposes
        let truncated_embedding: Vec<_> = embedding.iter().take(5).cloned().collect();

        println!("\nSentence: '{}'", sentences[i]);
        println!("Embedding (first 5 values): {:?}", truncated_embedding);
        println!("Embedding dimensions: {}", embedding.len());
    }

    Ok(())
}
```

</TabItem>
</Tabs> {/* Rust tabs */}

</TabItem>
</Tabs>
