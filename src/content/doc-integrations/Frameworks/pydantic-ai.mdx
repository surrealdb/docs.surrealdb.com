---
sidebar_position: 1
sidebar_label: Pydantic AI
title: Pydantic AI | Integrations
description: This section contains information about the Pydantic AI framework and how to integrate it with SurrealDB.
---

# Pydantic AI

[Pydantic AI](https://ai.pydantic.dev) is a Python agent framework designed to help you quickly, confidently, and painlessly build production grade applications and workflows with Generative AI.


## Example

This is a simple RAG application that uses Pydantic AI and embedded SurrealDB. The integration is done by providing the agent with a custom retrieval tool, which takes a search query, executes a SurrealDB vector-search query, and returns the results.

Find the full example in the [pydantic-ai-examples](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/rag_surrealdb.py) repository.

### Running the example

Set up your OpenAI API key:

```bash
export OPENAI_API_KEY=your-api-key
```

Or, store it in a .env file and add `--env-file .env` to your `uv run` commands.

Run with:

```bash
uv run -m pydantic_ai_examples.chat_app_surreal
```

### Code

```python
@dataclass
class Deps:
    openai: AsyncOpenAI
    db: AsyncWsSurrealConnection | AsyncHttpSurrealConnection


agent = Agent('openai:gpt-5', deps_type=Deps)


@agent.tool
async def retrieve(context: RunContext[Deps], search_query: str) -> str:
    """Retrieve documentation sections based on a search query.
    Args:
        context: The call context.
        search_query: The search query.
    """
    with logfire.span(
        'create embedding for {search_query=}', search_query=search_query
    ):
        embedding = await context.deps.openai.embeddings.create(
            input=search_query,
            model='text-embedding-3-small',
        )

    assert len(embedding.data) == 1, (
        f'Expected 1 embedding, got {len(embedding.data)}, doc query: {search_query!r}'
    )
    embedding_vector = embedding.data[0].embedding

    # SurrealDB vector search using HNSW index
    result = await context.deps.db.query(
        """
        SELECT url, title, content, vector::distance::knn() AS dist
        FROM doc_sections
        WHERE embedding <|8, 40|> $vector
        ORDER BY dist ASC
        LIMIT 8;
        """,
        {'vector': cast(Value, embedding_vector)},
    )

    # Process SurrealDB query result
    rows = []
    if isinstance(result, list):
        for record in result:
            if isinstance(record, dict) and 'url' in record:
                rows.append(record)

    return '\n\n'.join(
        f'# {row["title"]}\nDocumentation URL:{row["url"]}\n\n{row["content"]}\n'
        for row in rows
    )


async def run_agent(question: str):
    """Entry point to run the agent and perform RAG based question answering."""
    openai = AsyncOpenAI()
    logfire.instrument_openai(openai)

    logfire.info('Asking "{question}"', question=question)

    async with database_connect(False) as db:
        deps = Deps(openai=openai, db=db)
        answer = await agent.run(question, deps=deps)
    print(answer.output)
```
