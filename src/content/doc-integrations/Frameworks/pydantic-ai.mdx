---
sidebar_position: 1
sidebar_label: Pydantic AI
title: Pydantic AI | Integrations
description: This section contains information about the Pydantic AI framework and how to integrate it with SurrealDB.
---

# Pydantic AI

[Pydantic AI](https://ai.pydantic.dev) is a Python agent framework designed to help you quickly, confidently, and painlessly build production grade applications and workflows with Generative AI.


## Example

This is a simple RAG application that uses Pydantic AI and embedded SurrealDB. The integration is done by providing the agent with a custom retrieval tool, which takes a search query, executes a SurrealDB vector-search query, and returns the results.

Find the full example in the [pydantic-ai-examples](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/rag_surrealdb.py) repository.

### Running the example

Set up your OpenAI API key:

```bash
export OPENAI_API_KEY=your-api-key
```

Or, store it in a .env file and add `--env-file .env` to your `uv run` commands.

Build the vector store:

```bash
uv run --env-file .env -m pydantic_ai_examples.rag_surrealdb build
```

Run the agent:

```bash
uv run --env-file .env -m pydantic_ai_examples.rag_surrealdb search "How do I register a function as a custom tool for my agent?"
```

Or serve the web chat UI:

```bash
uv run --env-file .env uvicorn pydantic_ai_examples.rag_surrealdb:app --host 127.0.0.1 --port 7932
```

### Code

```python
embedder = Embedder('openai:text-embedding-3-small')
agent = Agent('openai:gpt-5')


@agent.tool_plain
async def retrieve(search_query: str) -> str:
    """Retrieve documentation sections based on a search query.

    Args:
        search_query: The search query.
    """

    @dataclass
    class RetrievalQueryResult:
        url: str
        title: str
        content: str
        dist: float

    result_ta = TypeAdapter(list[RetrievalQueryResult])

    with logfire.span(
        'create embedding for {search_query=}', search_query=search_query
    ):
        result = await embedder.embed_query(search_query)
        embedding = result.embeddings

    assert len(embedding) == 1, (
        f'Expected 1 embedding, got {len(embedding)}, doc query: {search_query!r}'
    )
    embedding_vector = list(embedding[0])

    # SurrealDB vector search using HNSW index
    async with database_connect(False) as db:
        result = await db.query(
            """
            SELECT url, title, content, vector::distance::knn() AS dist
            FROM doc_sections
            WHERE embedding <|8, 40|> $vector
            ORDER BY dist ASC
            """,
            {'vector': cast(Value, embedding_vector)},
        )

    # Process SurrealDB query result
    try:
        rows = result_ta.validate_python(result)
        logfire.info('Retrieved {len} results', len=len(rows))
    except Exception as e:
        logfire.error('Failed to validate JSON response: {error}', error=e)
        raise

    return '\n\n'.join(
        f'# {row.title}\nDocumentation URL:{row.url}\n\n{row.content}\n' for row in rows
    )


# Optionally, use this instead of the web chat UI
# async def run_agent(question: str):
#     """Entry point to run the agent and perform RAG based question answering."""
#     logfire.info('Asking "{question}"', question=question)
#     answer = await agent.run(question)
#     print(answer.output)


# Web chat UI
app = agent.to_web()
```
